{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import init_word_representations as iwr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens, U = iwr.get_pretrained_wordvec_from_file(\"./data/word_representations/glove.6B.50d.txt\", (400000, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SentCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for utterance and relation pair matching regression.\n",
    "    Uses an embedding layer, convolutional layer, max-pooling layer,\n",
    "    and a logistic regression layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 sequence_length, \n",
    "                 num_classes, \n",
    "                 init_embeddings, \n",
    "                 filter_sizes, \n",
    "                 num_filters,\n",
    "                 embeddings_trainable=False):\n",
    "        \"\"\"\n",
    "        :param sequence_length: The length of our sentences. Here we always pad\n",
    "        our sentences to have the same length (depending on the longest sentences\n",
    "        in our dataset).\n",
    "        :param num_classes: Number of classes in the output layer.\n",
    "        :param init_embeddings: Pre-trained word embeddings or initialied values.\n",
    "        :filter_sizes: The number of words we want our convolutional filters to cover. \n",
    "        We will have num_filters for each size specified here. For example, [3, 4, 5] \n",
    "        means that we will have filters that slide over 3, 4 and 5 words respectively, \n",
    "        for a total of 3 * num_filters filters.\n",
    "        :num_filters: The number of filters per filter size (see above).\n",
    "        :embeddings_trainable: Train embeddings or not.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Placeholders for input, output and dropout\n",
    "        \n",
    "        # input_x_u: batch_size x sequence_length\n",
    "        self.input_x_u = tf.placeholder(tf.int32, \n",
    "                                        [None, sequence_length],\n",
    "                                        name=\"input_x_u\")\n",
    "        # input_x_r: batch_size x num_classes x sequence_length\n",
    "        self.input_x_r = tf.placeholder(tf.int32, \n",
    "                                        [None, num_classes, sequence_length],\n",
    "                                        name=\"input_x_r\")\n",
    "        # input_y: batch_size, \n",
    "        self.input_y = tf.placeholder(tf.int64, \n",
    "                                      [None],\n",
    "                                      name=\"input_y\")\n",
    "        \n",
    "        # self.dropout_keep_prob = tf.placeholder(tf.float32, \n",
    "                                                #name=\"dropout_keep_prob\")\n",
    "        \n",
    "        self.embedding_size = np.shape(init_embeddings)[1]\n",
    "        \n",
    "        # Embedding layer\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            W = tf.Variable(init_embeddings,\n",
    "                            trainable=embeddings_trainable,\n",
    "                            name='W')\n",
    "            # batch_size x sequence_length x embedding_size\n",
    "            self.embedded_u = tf.nn.embedding_lookup(W, self.input_x_u)\n",
    "            print \"DEBUG: embedded_u -> %s\" % self.embedded_u\n",
    "            # batch_size x num_classes x sequence_length x embedding_size\n",
    "            self.embedded_r = tf.nn.embedding_lookup(W, self.input_x_r)\n",
    "            print \"DEBUG: embedded_r -> %s\" % self.embedded_r\n",
    "            # batch_size x sequence_length x embedding_size x 1\n",
    "            self.embedded_u_expanded = tf.expand_dims(self.embedded_u, -1)\n",
    "            print \"DEBUG: embedded_u_expanded -> %s\" % self.embedded_u_expanded\n",
    "            # batch_size x num_classes x sequence_length x embedding_size x 1\n",
    "            self.embedded_r_expanded = tf.expand_dims(self.embedded_r, -1)\n",
    "            print \"DEBUG: embedded_r_expanded -> %s\" % self.embedded_r_expanded\n",
    "        \n",
    "        # Create a convolution + maxpooling layer for each filter size\n",
    "        pooled_outputs_u = []\n",
    "        pooled_outputs_r = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s-u\" % filter_size):\n",
    "                # Convolution layer\n",
    "                filter_shape = [filter_size, self.embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), \n",
    "                                name='W')\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), \n",
    "                                name='b')\n",
    "                conv_u = tf.nn.conv2d(\n",
    "                    self.embedded_u_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv-u\")             \n",
    "                # Apply nonlinearity\n",
    "                h_u = tf.nn.relu(tf.nn.bias_add(conv_u, b), name=\"relu-u\")\n",
    "\n",
    "                # Maxpooling over outputs\n",
    "                pooled_u = tf.nn.max_pool(\n",
    "                    h_u,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"pool-u\")\n",
    "                pooled_outputs_u.append(pooled_u)\n",
    "                \n",
    "                # Pass each element in x_r through the same layer\n",
    "                pooled_outputs_r_wclasses = []\n",
    "                for j in range(num_classes):\n",
    "                    embedded_r = self.embedded_r_expanded[:, j, :, :, :]\n",
    "                    conv_r_j = tf.nn.conv2d(\n",
    "                        embedded_r,\n",
    "                        W, \n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding=\"VALID\",\n",
    "                        name=\"conv-r-%s\" % j)\n",
    "                    \n",
    "                    h_r_j = tf.nn.relu(tf.nn.bias_add(conv_r_j, b), name=\"relu-r-%s\" % j)\n",
    "                    \n",
    "                    pooled_r_j = tf.nn.max_pool(\n",
    "                        h_r_j,\n",
    "                        ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding=\"VALID\",\n",
    "                        name=\"pool-r-%s\" % j)\n",
    "                    pooled_outputs_r_wclasses.append(pooled_r_j)\n",
    "                # out_tensor: batch_size x 1 x num_class x num_filters\n",
    "                out_tensor = tf.concat(2, pooled_outputs_r_wclasses)\n",
    "                pooled_outputs_r.append(out_tensor)\n",
    "                    \n",
    "        \n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        print \"DEBUG: pooled_outputs_u -> %s\" % pooled_outputs_u\n",
    "        self.h_pool_u = tf.concat(3, pooled_outputs_u)\n",
    "        print \"DEBUG: h_pool_u -> %s\" % self.h_pool_u\n",
    "        # batch_size x 1 x num_filters_total\n",
    "        self.h_pool_flat_u = tf.reshape(self.h_pool_u, [-1, 1, num_filters_total])\n",
    "        print \"DEBUG: h_pool_flat_u -> %s\" % self.h_pool_flat_u\n",
    "        \n",
    "        \n",
    "        print \"DEBUG: pooled_outputs_r -> %s\" % pooled_outputs_r\n",
    "        self.h_pool_r = tf.concat(3, pooled_outputs_r)\n",
    "        print \"DEBUG: h_pool_r -> %s\" % self.h_pool_r\n",
    "        # h_pool_flat_r: batch_size x num_classes X num_filters_total\n",
    "        self.h_pool_flat_r = tf.reshape(self.h_pool_u, [-1, num_classes, num_filters_total])\n",
    "        print \"DEBUG: h_pool_flat_r -> %s\" % self.h_pool_flat_r\n",
    "        \n",
    "        # Add dropout\n",
    "        # with tf.name_scope(\"dropout\"):\n",
    "        #    self.h_drop_u = tf.nn.dropout(self.h_pool_flat_u, self.dropout_keep_prob)\n",
    "        \n",
    "        # cosine layer - final scores and predictions\n",
    "        with tf.name_scope(\"cosine_layer\"):\n",
    "            dot =  tf.reduce_sum(tf.mul(self.h_pool_flat_u, \n",
    "                                        self.h_pool_flat_r), 2)\n",
    "            print \"DEBUG: dot -> %s\" % dot\n",
    "            sqrt_u = tf.sqrt(tf.reduce_sum(self.h_pool_flat_u**2, 2))\n",
    "            print \"DEBUG: sqrt_u -> %s\" % sqrt_u\n",
    "            sqrt_r = tf.sqrt(tf.reduce_sum(self.h_pool_flat_r**2, 2))\n",
    "            print \"DEBUG: sqrt_r -> %s\" % sqrt_r\n",
    "            self.cosine = dot / (sqrt_u * sqrt_r)\n",
    "            print \"DEBUG: cosine -> %s\" % self.cosine\n",
    "            self.predictions = tf.argmax(self.cosine, 1, name=\"predictions\")\n",
    "            print \"DEBUG: predictions -> %s\" % self.predictions\n",
    "        \n",
    "        # softmax regression - loss and prediction\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(self.cosine, self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses)\n",
    "            \n",
    "        # Calculate Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, self.input_y)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: embedded_u -> Tensor(\"embedding_23/embedding_lookup:0\", shape=(?, 3, 5), dtype=float32)\n",
      "DEBUG: embedded_r -> Tensor(\"embedding_23/embedding_lookup_1:0\", shape=(?, 4, 3, 5), dtype=float32)\n",
      "DEBUG: embedded_u_expanded -> Tensor(\"embedding_23/ExpandDims:0\", shape=(?, 3, 5, 1), dtype=float32)\n",
      "DEBUG: embedded_r_expanded -> Tensor(\"embedding_23/ExpandDims_1:0\", shape=(?, 4, 3, 5, 1), dtype=float32)\n",
      "DEBUG: pooled_outputs_u -> [<tf.Tensor 'conv-maxpool-2-u_28/pool-u:0' shape=(?, 1, 1, 10) dtype=float32>, <tf.Tensor 'conv-maxpool-2-u_29/pool-u:0' shape=(?, 1, 1, 10) dtype=float32>]\n",
      "DEBUG: h_pool_u -> Tensor(\"concat_18:0\", shape=(?, 1, 1, 20), dtype=float32)\n",
      "DEBUG: h_pool_flat_u -> Tensor(\"Reshape_13:0\", shape=(?, 1, 20), dtype=float32)\n",
      "DEBUG: pooled_outputs_r -> [<tf.Tensor 'conv-maxpool-2-u_28/concat:0' shape=(?, 1, 4, 10) dtype=float32>, <tf.Tensor 'conv-maxpool-2-u_29/concat:0' shape=(?, 1, 4, 10) dtype=float32>]\n",
      "DEBUG: h_pool_r -> Tensor(\"concat_19:0\", shape=(?, 1, 4, 20), dtype=float32)\n",
      "DEBUG: h_pool_flat_r -> Tensor(\"Reshape_14:0\", shape=(?, 4, 20), dtype=float32)\n",
      "DEBUG: dot -> Tensor(\"cosine_layer_4/Sum:0\", shape=(?, 4), dtype=float32)\n",
      "DEBUG: sqrt_u -> Tensor(\"cosine_layer_4/Sqrt:0\", shape=(?, 1), dtype=float32)\n",
      "DEBUG: sqrt_r -> Tensor(\"cosine_layer_4/Sqrt_1:0\", shape=(?, 4), dtype=float32)\n",
      "DEBUG: cosine -> Tensor(\"cosine_layer_4/div:0\", shape=(?, 4), dtype=float32)\n",
      "DEBUG: predictions -> Tensor(\"cosine_layer_4/predictions:0\", shape=(?,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "init_embeddings=[[0.111, 0.919, 0.818, 0.717, 0.616],\n",
    "                 [0.212, 0.111, 0.118, 0.919, 0.515],\n",
    "                 [0.313, 0.212, 0.217, 0.811, 0.414],\n",
    "                 [0.414, 0.313, 0.316, 0.712, 0.313],\n",
    "                 [0.515, 0.414, 0.515, 0.613, 0.212],\n",
    "                 [0.616, 0.717, 0.818, 0.919, 0.111]]\n",
    "\n",
    "cnn = SentCNN(sequence_length=3, \n",
    "              num_classes=4, \n",
    "              init_embeddings=init_embeddings, \n",
    "              filter_sizes=[2, 2], \n",
    "              num_filters=10,\n",
    "              embeddings_trainable=False)\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception AssertionError: AssertionError() in <bound method InteractiveSession.__del__ of <tensorflow.python.client.session.InteractiveSession object at 0x7f7f91083750>> ignored\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_step(x_u_batch, x_r_batch, y_batch):\n",
    "    \"\"\"\n",
    "    A single training step.\n",
    "    \"\"\"\n",
    "    feed_dict = {\n",
    "        \n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
