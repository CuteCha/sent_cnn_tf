{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "need more than 2 values to unpack",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-35463939c5fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m## TEST get_training_examples FUCNTION\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0mx_u\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_r\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miwr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_training_examples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./data/webquestions.examples.train.e2e.top10.filter.patrel.sid.tsv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: need more than 2 values to unpack"
     ]
    }
   ],
   "source": [
    "# TEST init_word_representations.py MODULE\n",
    "import init_word_representations as iwr\n",
    "\n",
    "## TEST get_training_examples FUCNTION\n",
    "(x_u, x_r, y, max_len) = iwr.get_training_examples(\"./data/webquestions.examples.train.e2e.top10.filter.patrel.sid.tsv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from init_word_representations import clean_str\n",
    "def get_training_examples_raw(training_file_name, n_neg=5000):\n",
    "    \"\"\"\n",
    "    Load training data file, and split the data into words and labels.\n",
    "    Return utterances, relation words, \n",
    "    labels and the largest length of training sentences.\n",
    "    \"\"\"\n",
    "    positive_utterances = []\n",
    "    positive_relations = []\n",
    "    negative_utterances = []\n",
    "    negative_relations = []\n",
    "    max_len = 0\n",
    "    with open(training_file_name) as tf:\n",
    "        for lines in tf:\n",
    "            # data -> [f1, u_str, r_str, WQ_NUM]\n",
    "            data = lines.split('\\t')\n",
    "            f1 = float(data[0])\n",
    "            u_str = clean_str(data[1].strip()).split(\" \")\n",
    "            r_str = clean_str(data[2].strip()).split(\" \")\n",
    "            max_len = max(len(u_str), len(r_str), max_len)\n",
    "            if f1 >= 0.5:\n",
    "                positive_utterances.append(u_str)\n",
    "                positive_relations.append(r_str)\n",
    "            elif f1 == 0:\n",
    "                negative_utterances.append(u_str)\n",
    "                negative_relations.append(r_str)\n",
    "        samples = np.random.choice(len(negative_utterances), n_neg, replace=False)\n",
    "        negative_utterances = [negative_utterances[i] for i in samples]\n",
    "        negative_relations = [negative_relations[i] for i in samples]\n",
    "        x_u = positive_utterances + negative_utterances\n",
    "        x_r = positive_relations + negative_relations\n",
    "        positive_labels = np.ones(len(positive_utterances))\n",
    "        negative_labels = np.zeros(len(negative_utterances))\n",
    "        y = np.concatenate((positive_labels, negative_labels))\n",
    "    \n",
    "    return (x_u, x_r, y, max_len)\n",
    "\n",
    "## TEST get_training_examples FUCNTION\n",
    "(x_u, x_r, y, max_len) = get_training_examples(\"./data/webquestions.examples.train.e2e.top10.filter.patrel.sid.tsv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['where', 'was', 'e1', 'when', 'he', 'died'],\n",
       " ['base', 'kwebbase', 'kwtopic', 'disciplines'],\n",
       " 0.0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_u[7100], x_r[7100], y[7100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from init_word_representations import clean_str\n",
    "def get_training_examples_for_softmax(training_file_name, n_neg_sample=5):\n",
    "    \"\"\"\n",
    "    Load training data file, and split the data into words and labels.\n",
    "    Return utterances, relation words, \n",
    "    labels and the largest length of training sentences.\n",
    "    The output of this funtion is formatted for softmax regression.\n",
    "    \"\"\"\n",
    "    neg_dict = {}\n",
    "    max_len = 0\n",
    "    wqn_lst = []\n",
    "    x_u = []\n",
    "    x_r = []\n",
    "    y = []\n",
    "    with open(training_file_name) as tf:\n",
    "        for lines in tf:\n",
    "            # data -> [f1, u_str, r_str, WQ_NUM]\n",
    "            data = lines.split('\\t')\n",
    "            f1 = float(data[0])\n",
    "            u_str = clean_str(data[1].strip()).split(\" \")\n",
    "            r_str = clean_str(data[2].strip()).split(\" \")\n",
    "            wqn = data[3].strip()\n",
    "            max_len = max(len(u_str), len(r_str), max_len)\n",
    "            \n",
    "            if f1 >= 0.5:\n",
    "                wqn_lst.append(wqn)\n",
    "                x_u.append(u_str)\n",
    "                x_r.append([r_str])\n",
    "            else:\n",
    "                if neg_dict.has_key(wqn):\n",
    "                    if len(neg_dict[wqn]) < n_neg_sample:\n",
    "                        neg_dict[wqn].append(r_str)\n",
    "                else:\n",
    "                    neg_dict[wqn] = [r_str]\n",
    "        for i in range(len(x_u)):\n",
    "            if not neg_dict.has_key(wqn_lst[i]):\n",
    "                neg_dict[wqn_lst[i]] = neg_dict[wqn_lst[0]]\n",
    "            if len(neg_dict[wqn_lst[i]]) < n_neg_sample:\n",
    "                neg_dict[wqn_lst[i]] += neg_dict[wqn_lst[0]][:n_neg_sample - len(neg_dict[wqn_lst[i]])]\n",
    "            if len(neg_dict[wqn_lst[i]]) != n_neg_sample:\n",
    "                print neg_dict[wqn_lst[i]]\n",
    "            x_r[i] = x_r[i] + neg_dict[wqn_lst[i]]\n",
    "            # add a little randomness \n",
    "            y.append(np.random.randint(len(x_r[i])))\n",
    "            tmp = x_r[i][0]\n",
    "            x_r[i][0] = x_r[i][y[i]]\n",
    "            x_r[i][y[i]] = tmp\n",
    "    \n",
    "    return (x_u, x_r, y, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(x_u, x_r, y, max_len) = get_training_examples_for_softmax(\"./data/webquestions.examples.train.e2e.top10.filter.patrel.sid.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = tf.Variable([[[1.0,2.0,3.0],[4.0,5.0,6.0],[7.0,8.0,9.0]],[[1.0,2.0,3.0],[4.0,5.0,6.0],[11.0,11.0,11.0]]])\n",
    "b = tf.constant([[1.0,2.0,3.0],[-1.0,2.0,-3.0]])\n",
    "c = tf.reshape(b, [2, 1, 3])\n",
    "a.initializer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3.74165726   8.77496433  13.92838669]\n",
      " [  3.74165726   8.77496433  19.0525589 ]]\n",
      "[[ 3.7416575]\n",
      " [ 3.7416575]]\n",
      "[[ 1.          0.97463179  0.95941204]\n",
      " [-0.42857143 -0.36548692 -0.30860671]]\n"
     ]
    }
   ],
   "source": [
    "print tf.sqrt(tf.reduce_sum(a**2, 2)).eval()\n",
    "print tf.sqrt(tf.reduce_sum(c**2, 2)).eval()\n",
    "d = tf.sqrt(tf.reduce_sum(a**2, 2)) * tf.sqrt(tf.reduce_sum(c**2, 2))\n",
    "\n",
    "print (tf.reduce_sum(tf.mul(a,c), 2) / d).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
